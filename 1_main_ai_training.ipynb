{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50f35c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import models\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from scipy import ndimage\n",
    "import cv2\n",
    "# (Keep your other imports, e.g., for transforms, if needed)\n",
    "\n",
    "class type_dataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, augmentation=False, crop_augmentation=False, noise=False):\n",
    "        self.data = []\n",
    "        self.transform = transform\n",
    "        self.augmentation = augmentation\n",
    "        self.crop_augmentation = crop_augmentation\n",
    "        self.noise = noise\n",
    "\n",
    "        pathways = ['apoptosis', 'necroptosis', 'necrosis', 'control', 'treatedalive']\n",
    "        pathways_folder = ['0_Apoptosis', '0_Necroptosis', '0_Necrosis', '0_Control', '0_TreatedAlive']\n",
    "        # pathways_folder = ['background', 'necrosis']\n",
    "        for pathway_label, folder in enumerate(pathways_folder):\n",
    "            pathway_dir = os.path.join(root_dir, folder)\n",
    "            for fname in os.listdir(pathway_dir):\n",
    "                image_path = os.path.join(pathway_dir, fname)\n",
    "                image = Image.open(image_path)\n",
    "                image = np.array(image)\n",
    "                # image = cv2.equalizeHist(image)\n",
    "                self.data.append((image, pathway_label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the primary image and label\n",
    "        image, label = self.data[idx]\n",
    "        \n",
    "        # Convert to tensor and repeat for 3 channels if necessary\n",
    "        image_tensor = torch.from_numpy(image).repeat(3, 1, 1).float()\n",
    "        \n",
    "        # Apply any prebuilt transform\n",
    "        flips = transforms.Compose([\n",
    "                                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                                transforms.RandomVerticalFlip(p=0.5)\n",
    "                            ])\n",
    "        if self.crop_augmentation:\n",
    "            crop_size = random.randint(160, 320)\n",
    "            top = random.randint(0, 320 - crop_size)\n",
    "            left = random.randint(0, 320 - crop_size)\n",
    "            image_tensor = image_tensor[:, top:top+crop_size, left:left+crop_size]\n",
    "\n",
    "        if self.transform:\n",
    "            image_tensor = self.transform(image_tensor)\n",
    "        \n",
    "        if self.noise:\n",
    "            noise_std = 0.1\n",
    "            noise = torch.randn_like(image_tensor) * noise_std\n",
    "            image_tensor = image_tensor + noise\n",
    "        \n",
    "        if self.augmentation:\n",
    "            # ---- CutMix Augmentation Start ----\n",
    "            image_tensor = flips(image_tensor)\n",
    "            # Add Gaussian noise (mean=0, std_dev=some_value, e.g. 0.1)\n",
    "            noise_std = 0.1\n",
    "            noise = torch.randn_like(image_tensor) * noise_std\n",
    "            image_tensor = image_tensor + noise\n",
    "            \n",
    "            # Sample a random second image (optionally ensure it's different from idx)\n",
    "            rand_idx = random.randint(0, len(self.data) - 1)\n",
    "            image2, label2 = self.data[rand_idx]\n",
    "\n",
    "            \"\"\"\n",
    "            if self.noise:\n",
    "                image2 = ndimage.gaussian_filter(image2, sigma=1)\n",
    "            \"\"\"\n",
    "            image_tensor2 = torch.from_numpy(image2).repeat(3, 1, 1).float()\n",
    "            if self.crop_augmentation:\n",
    "                image_tensor2 = image_tensor2[:, top:top+crop_size, left:left+crop_size]\n",
    "            \n",
    "            if self.transform:\n",
    "                image_tensor2 = self.transform(image_tensor2)\n",
    "            image_tensor2 = flips(image_tensor2)\n",
    "            \n",
    "            \n",
    "            if self.noise:\n",
    "                noise_std = 0.1\n",
    "                noise = torch.randn_like(image_tensor2) * noise_std\n",
    "                image_tensor2 = image_tensor2 + noise\n",
    "            \n",
    "            # Sample lambda from a Beta distribution (here using alpha=1.0 for both sides)\n",
    "            lam = np.random.beta(1.0, 1.0)\n",
    "            # Get image dimensions (assuming images are of equal size)\n",
    "            _, H, W = image_tensor.size()\n",
    "            # Compute the size of the patch to cut and paste\n",
    "            r = np.sqrt(1 - lam)\n",
    "            cut_w = int(W * r)\n",
    "            cut_h = int(H * r)\n",
    "            \n",
    "            # Choose a random center point for the patch\n",
    "            cx = np.random.randint(W)\n",
    "            cy = np.random.randint(H)\n",
    "            \n",
    "            # Calculate the bounding box coordinates and clip to image size\n",
    "            x1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "            y1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "            x2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "            y2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "            \n",
    "            # Replace the region in image_tensor with the corresponding patch from image_tensor2\n",
    "            image_tensor[:, y1:y2, x1:x2] = image_tensor2[:, y1:y2, x1:x2]\n",
    "            \n",
    "            # Adjust lambda to exactly match the pixel ratio of the mixed region\n",
    "            lam = 1 - ((x2 - x1) * (y2 - y1) / (W * H))\n",
    "            # Return a tuple for the label: (label_a, label_b, lam)\n",
    "            label = (label, label2, lam)\n",
    "            # ---- CutMix Augmentation End ----\n",
    "            \n",
    "        return image_tensor, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d8a69ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.AugmentedDatasetWrapper import AugmentedDatasetWrapper\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models\n",
    "\n",
    "train_dir = \"C:/rkka_Projects/cell_death_v2/Data/model_training/mip/train\"\n",
    "val_dir = \"C:/rkka_Projects/cell_death_v2/Data/model_training/mip/test\"\n",
    "\n",
    "transform = models.ResNet50_Weights.IMAGENET1K_V2.transforms()\n",
    "train_dataset = type_dataset(train_dir, transform=transform, augmentation=True, crop_augmentation=True, noise=True)\n",
    "val_dataset = type_dataset(val_dir, transform=transform, augmentation=False, crop_augmentation=False)\n",
    "augmented_train_dataset = AugmentedDatasetWrapper(dataset=train_dataset, num_repeats=6)\n",
    "augmented_val_dataset = AugmentedDatasetWrapper(dataset=train_dataset, num_repeats=1)\n",
    "\n",
    "train_loader = DataLoader(dataset=augmented_train_dataset, shuffle=True, batch_size=64)\n",
    "val_loader = DataLoader(dataset=val_dataset, shuffle=True, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da83f03e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\miniconda3\\envs\\cell\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\miniconda3\\envs\\cell\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 23,518,277\n",
      "Trainable Parameters: 16,081,920\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "import torch\n",
    "\n",
    "model = models.resnet50(pretrained=True)\n",
    "num_features = model.fc.in_features\n",
    "model.fc = torch.nn.Sequential(\n",
    "    torch.nn.Dropout(0.2),\n",
    "    torch.nn.Linear(num_features, 5)\n",
    ")\n",
    "model.load_state_dict(torch.load(r\"C:\\rkka_Projects\\cell_death_v2\\trained_models\\ai_epoch_33_24.242485_0.9728.pth\"))\n",
    "for name, params in model.named_parameters():\n",
    "    if 'layer4' in name or 'layer3.5' in name:\n",
    "        params.requires_grad = True\n",
    "    else:\n",
    "        params.requires_grad = False\n",
    "\n",
    "        \n",
    "utils.print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37d35ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase the weight for classes 3 and 4; adjust values as needed\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.00005)\n",
    "\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33401247",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/150 [00:13<33:04, 13.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0\n",
      "train loss : 24.125369\n",
      "val loss : 0.519867 || val_acc : 0.9388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 2/150 [00:27<33:26, 13.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1\n",
      "train loss : 23.536839\n",
      "val loss : 0.495792 || val_acc : 0.9660\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3/150 [00:40<32:55, 13.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 2\n",
      "train loss : 23.657306\n",
      "val loss : 0.529723 || val_acc : 0.9592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 4/150 [00:54<32:55, 13.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 3\n",
      "train loss : 23.608289\n",
      "val loss : 0.513320 || val_acc : 0.9660\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 5/150 [01:07<32:31, 13.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 4\n",
      "train loss : 23.375250\n",
      "val loss : 0.591015 || val_acc : 0.9660\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 6/150 [01:20<32:21, 13.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 5\n",
      "train loss : 23.725262\n",
      "val loss : 0.549549 || val_acc : 0.9660\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 7/150 [01:34<32:14, 13.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 6\n",
      "train loss : 24.070198\n",
      "val loss : 0.528996 || val_acc : 0.9592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 8/150 [01:47<31:48, 13.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 7\n",
      "train loss : 23.477241\n",
      "val loss : 0.492418 || val_acc : 0.9524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 9/150 [02:01<31:38, 13.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 8\n",
      "train loss : 23.896513\n",
      "val loss : 0.472503 || val_acc : 0.9524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 10/150 [02:15<31:38, 13.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 9\n",
      "train loss : 23.042517\n",
      "val loss : 0.614250 || val_acc : 0.9524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 11/150 [02:28<31:14, 13.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 10\n",
      "train loss : 22.959663\n",
      "val loss : 0.534966 || val_acc : 0.9456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 12/150 [02:41<30:58, 13.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 11\n",
      "train loss : 23.662053\n",
      "val loss : 0.698275 || val_acc : 0.9456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 13/150 [02:55<30:48, 13.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 12\n",
      "train loss : 23.417795\n",
      "val loss : 0.610718 || val_acc : 0.9524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 14/150 [03:08<30:36, 13.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 13\n",
      "train loss : 23.606480\n",
      "val loss : 0.563357 || val_acc : 0.9388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 16/150 [03:26<25:42, 11.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 15\n",
      "train loss : 23.337025\n",
      "val loss : 0.458064 || val_acc : 0.9592\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "\n",
    "for epoch in tqdm(range(150)):\n",
    "    # train\n",
    "    model.train()\n",
    "    train_loss, train_correct, train_total = 0, 0, 0\n",
    "    try:\n",
    "        for images, labels in train_loader:\n",
    "            images = images.cuda()\n",
    "            \n",
    "            # Move each component of labels to the GPU if using CutMix\n",
    "            if isinstance(labels, (list, tuple)):\n",
    "                label_a, label_b, lam = labels\n",
    "                label_a = label_a.cuda()\n",
    "                label_b = label_b.cuda()\n",
    "                lam     = lam.cuda()\n",
    "            else:\n",
    "                labels = labels.cuda()\n",
    "            \n",
    "            outputs = model(images)\n",
    "            \n",
    "            if isinstance(labels, (list, tuple)):\n",
    "                # Compute per-sample losses by setting reduction='none'\n",
    "                loss_a = torch.nn.functional.cross_entropy(outputs, label_a, reduction='none')\n",
    "                loss_b = torch.nn.functional.cross_entropy(outputs, label_b, reduction='none')\n",
    "                # Combine losses per sample and then average to get a scalar loss\n",
    "                loss = (lam * loss_a + (1 - lam) * loss_b).mean()\n",
    "            else:\n",
    "                loss = criterion(outputs, labels)  # This remains as is if no CutMix is used\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            \n",
    "        #validation\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for val_images, val_labels in val_loader:\n",
    "                val_images, val_labels = val_images.cuda(), val_labels.cuda()\n",
    "                    \n",
    "                outputs = model(val_images)\n",
    "                loss = criterion(outputs, val_labels)\n",
    "                val_loss += loss\n",
    "                    \n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                val_correct += (preds==val_labels).sum().item()\n",
    "                val_total += len(val_labels)\n",
    "        \n",
    "        torch.save(model.state_dict(), f'trained_models/ai_epoch_{epoch+33}_{train_loss:.6f}_{val_correct/val_total:.4f}.pth')\n",
    "        \n",
    "        print(f\"Epoch : {epoch}\")\n",
    "        print(f\"train loss : {train_loss:.6f}\")\n",
    "        print(f\"val loss : {val_loss:.6f} || val_acc : {val_correct/val_total:.4f}\")\n",
    "    except:\n",
    "        pass\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cell",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
