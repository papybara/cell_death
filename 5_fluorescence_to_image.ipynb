{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bb583c8",
   "metadata": {},
   "source": [
    "<h1> Inference First </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c7f937a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from TCFile import TCFile\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "from torchvision import models\n",
    "import utils\n",
    "\n",
    "def process(path, time, model, mini_model, crop_size=(160,160), adaptive_crop=False, overlap=True, stride_proportion=0.5,\n",
    "            wanted_patches=[], background_vote=False):\n",
    "    # Load                                                                                    & prep data\n",
    "    file = TCFile(path, '2DMIP')\n",
    "    crop_size = crop_size\n",
    "\n",
    "    slice_2d = utils.resize_tomogram_mip(\n",
    "        file[time],\n",
    "        data_resolution=file.data_resolution,\n",
    "        target_resolution=0.1632,\n",
    "        mode='mip'\n",
    "    )\n",
    "\n",
    "    if adaptive_crop:\n",
    "        # Adjust the patch size if you want the patches to fit the image nicely\n",
    "        residual = slice_2d.shape[0] % crop_size[0]\n",
    "        fit = slice_2d.shape[0] // crop_size[0]\n",
    "        adapt = residual // fit if fit > 0 else 0\n",
    "        crop_size = (crop_size[0] + adapt, crop_size[1] + adapt)\n",
    "        stride = int(crop_size[0]*stride_proportion)\n",
    "\n",
    "    base_image = slice_2d.copy()\n",
    "    # base_image = utils.image_normalization(base_image, min=1.33, max=1.4)\n",
    "\n",
    "    # Normal PyTorch model prep\n",
    "    transform = models.ResNet101_Weights.IMAGENET1K_V2.transforms()\n",
    "    mini_transform = models.ResNet50_Weights.IMAGENET1K_V2.transforms()\n",
    "\n",
    "    model = model.cuda().eval()\n",
    "    mini_model = mini_model.cuda().eval()\n",
    "\n",
    "    # We'll store class votes for each pixel here: (H, W, num_classes)\n",
    "    num_classes = 1+5  # Adjust if you actually have a different number\n",
    "    label_counts = np.zeros((base_image.shape[0], base_image.shape[1], num_classes), dtype=np.int32)\n",
    "    patch_layer = np.zeros((base_image.shape[0], base_image.shape[1]), dtype=np.int32)\n",
    "    \n",
    "    patch_coord = []\n",
    "    wanted_patches = wanted_patches\n",
    "    wanted_probabilities = []\n",
    "    wanted_images = []\n",
    "    \n",
    "    if overlap:\n",
    "        # Overlapping patches: define a smaller stride\n",
    "        # e.g. half the patch size in each dimension\n",
    "        stride_h = stride_w = stride\n",
    "        patch_num = 0\n",
    "\n",
    "        for top in range(0, base_image.shape[0] - crop_size[0] + 1, stride_h):\n",
    "            for left in range(0, base_image.shape[1] - crop_size[1] + 1, stride_w):\n",
    "                patch_coord.append((top, left, crop_size[0]))\n",
    "                # Extract patch\n",
    "                cropped = base_image[top:top+crop_size[0], left:left+crop_size[1]]\n",
    "\n",
    "                # Clamp intensities (like your code)\n",
    "                cropped = np.clip(cropped, 1.33, 1.40)\n",
    "                cropped = utils.image_normalization(cropped, min=1.33, max=1.40)\n",
    "\n",
    "                # Decide if we run the main model or the mini_model logic\n",
    "                temp = cropped.copy()\n",
    "                temp[temp < 45] = 0\n",
    "                temp[temp > 45] = 1\n",
    "                proportion = np.count_nonzero(temp == 1) / (np.count_nonzero(temp == 0) + np.count_nonzero(temp == 1))\n",
    "\n",
    "                if proportion < 0.05:\n",
    "                    # Use mini_model to check necrosis\n",
    "                    mini_temp = cv2.equalizeHist(cropped.astype(np.uint8))\n",
    "                    mini_temp = torch.from_numpy(mini_temp).repeat(3, 1, 1).float()\n",
    "                    mini_temp = mini_transform(mini_temp).cuda()\n",
    "\n",
    "                    is_necrosis = torch.max(mini_model(mini_temp.unsqueeze(0)), 1)[1].item()\n",
    "\n",
    "                    if is_necrosis == 1:\n",
    "                        # If necrosis, run the main model\n",
    "                        image_tensor = torch.from_numpy(cropped).repeat(3, 1, 1).float()\n",
    "                        image_tensor = transform(image_tensor).cuda()\n",
    "                        output = model(image_tensor.unsqueeze(0)).cpu()\n",
    "                        _, pred = torch.max(output, 1)\n",
    "                        class_label = pred.item() + 1  # 0..(num_classes-1)\n",
    "                    else:\n",
    "                        class_label = 0  # or some \"background\" label\n",
    "                else:\n",
    "                    # Directly run main model\n",
    "                    image_tensor = torch.from_numpy(cropped).repeat(3, 1, 1).float()\n",
    "                    image_tensor = transform(image_tensor).cuda()\n",
    "                    output = model(image_tensor.unsqueeze(0)).cpu()\n",
    "                    _, pred = torch.max(output, 1)\n",
    "                    class_label = pred.item() + 1\n",
    "                    \n",
    "                \n",
    "                if patch_num in wanted_patches:\n",
    "                    wanted_probabilities.append(output.detach().numpy())\n",
    "                    wanted_images.append(base_image[top:top+crop_size[0], left:left+crop_size[1]])\n",
    "\n",
    "                # Add a vote for this class in label_counts\n",
    "                label_counts[top:top+crop_size[0], left:left+crop_size[1], class_label] += 1\n",
    "                patch_layer[top:top+crop_size[0], left:left+crop_size[1]] = patch_num\n",
    "                patch_num += 1\n",
    "                \n",
    "    else:\n",
    "        # Original no-overlap approach\n",
    "        patches = utils.crop_patch(slice_2d, crop_size=crop_size, overlap=False)\n",
    "        patch_num = 0\n",
    "        for patch_index, patch in enumerate(patches):\n",
    "            top, left = patch[0], patch[1]\n",
    "            patch_coord.append((top, left, crop_size[0]))\n",
    "            cropped = base_image[top:top+crop_size[0], left:left+crop_size[1]]\n",
    "\n",
    "            cropped = np.clip(cropped, 1.33, 1.40)\n",
    "            cropped = utils.image_normalization(cropped, min=1.33, max=1.40)\n",
    "\n",
    "            temp = cropped.copy()\n",
    "            temp[temp < 45] = 0\n",
    "            temp[temp > 45] = 1\n",
    "            proportion = np.count_nonzero(temp == 1) / (np.count_nonzero(temp == 0) + np.count_nonzero(temp == 1))\n",
    "\n",
    "            if proportion < 0.05:\n",
    "                mini_temp = cv2.equalizeHist(cropped.astype(np.uint8))\n",
    "                mini_temp = torch.from_numpy(mini_temp).repeat(3, 1, 1).float()\n",
    "                mini_temp = mini_transform(mini_temp).cuda()\n",
    "                is_necrosis = torch.max(mini_model(mini_temp.unsqueeze(0)), 1)[1].item()\n",
    "\n",
    "                if is_necrosis == 1:\n",
    "                    image_tensor = torch.from_numpy(cropped).repeat(3, 1, 1).float()\n",
    "                    image_tensor = transform(image_tensor).cuda()\n",
    "                    output = model(image_tensor.unsqueeze(0)).cpu()\n",
    "                    _, pred = torch.max(output, 1)\n",
    "                    class_label = pred.item() + 1\n",
    "                else:\n",
    "                    class_label = 0\n",
    "            else:\n",
    "                image_tensor = torch.from_numpy(cropped).repeat(3, 1, 1).float()\n",
    "                image_tensor = transform(image_tensor).cuda()\n",
    "                output = model(image_tensor.unsqueeze(0)).cpu()\n",
    "                _, pred = torch.max(output, 1)\n",
    "                class_label = pred.item() + 1\n",
    "                \n",
    "                \n",
    "            if patch_num in wanted_patches:\n",
    "                wanted_probabilities.append(output.detach().numpy())\n",
    "                wanted_images.append(base_image[top:top+crop_size[0], left:left+crop_size[1]])\n",
    "                \n",
    "            label_counts[top:top+crop_size[0], left:left+crop_size[1], class_label] += 1\n",
    "            patch_layer[top:top+crop_size[0], left:left+crop_size[1]] = patch_num\n",
    "            patch_num += 1\n",
    "    # Final pixelwise label = most frequent vote + 1 (if you want 1-based classes)\n",
    "    # label_image = np.argmax(label_counts, axis=-1)\n",
    "    \n",
    "    # --- MODIFIED FINAL LABEL ASSIGNMENT ---\n",
    "    # Final pixelwise label assignment with background priority\n",
    "\n",
    "    # Calculate the standard argmax first (most frequent vote overall)\n",
    "    provisional_label_image = np.argmax(label_counts, axis=-1)\n",
    "\n",
    "    if background_vote:\n",
    "        # Identify pixels where at least one patch voted for background (class 0)\n",
    "        # label_counts[:, :, 0] accesses the counts for class 0 for all pixels\n",
    "        total = np.sum(label_counts, axis=2)\n",
    "        has_background_vote = 0.15 < label_counts[:, :, 0]/total\n",
    "\n",
    "        # Initialize the final label image with the provisional result\n",
    "        label_image = provisional_label_image.copy()\n",
    "\n",
    "        # Override: wherever a background vote exists, set the final label to 0\n",
    "        label_image[has_background_vote] = 0\n",
    "        # --- END OF MODIFIED SECTION ---\n",
    "    else:\n",
    "        label_image = provisional_label_image.copy()\n",
    "\n",
    "    return base_image, label_image, patch_layer, patch_coord, wanted_probabilities, wanted_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "195852f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\miniconda3\\envs\\cell\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\miniconda3\\envs\\cell\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "c:\\miniconda3\\envs\\cell\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "  0%|          | 0/73 [00:00<?, ?it/s]C:\\Users\\mw\\AppData\\Local\\Temp\\ipykernel_24300\\238050702.py:165: RuntimeWarning: invalid value encountered in divide\n",
      "  has_background_vote = 0.15 < label_counts[:, :, 0]/total\n",
      "100%|██████████| 73/73 [00:12<00:00,  5.90it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torchvision import models\n",
    "import utils\n",
    "import numpy as np\n",
    "\n",
    "path = r\"C:\\rkka_Projects\\cell_death_v2\\Data\\9. A549_FasL(20250410)\\250409.170229.A549_FasL_01.025.Group3.B5.T025P03.TCF\"\n",
    "class_num = 5\n",
    "model_path = r\"C:\\rkka_Projects\\cell_death_v2\\trained_models\\test_5_classes_22.032991_0.9728_sota.pth\"\n",
    "mini_model_path = r\"C:\\rkka_Projects\\cell_death_v2\\trained_models\\mini_ai_epoch_9_0.000861_1.0000.pth\"\n",
    "file = TCFile(path, '2DMIP')\n",
    "# Load Model\n",
    "model = models.resnet101(pretrained=True)\n",
    "num_features = model.fc.in_features\n",
    "model.fc = torch.nn.Sequential(\n",
    "    torch.nn.Dropout(0.2),\n",
    "    torch.nn.Linear(num_features, 5)\n",
    ")\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "# Load mini Model\n",
    "mini_model = models.resnet50(pretrained=True)\n",
    "num_features = mini_model.fc.in_features\n",
    "mini_model.fc = torch.nn.Sequential(\n",
    "    torch.nn.Dropout(0.2),\n",
    "    torch.nn.Linear(num_features, 2)\n",
    ")\n",
    "mini_model.load_state_dict(torch.load(mini_model_path))\n",
    "\n",
    "# Process\n",
    "base_stack = []\n",
    "label_stack = []\n",
    "probabilities_stack = []\n",
    "\n",
    "# wanted_patches = [110, 334, 329, 258, 192]\n",
    "# wanted_patches = [0]\n",
    "\n",
    "# for i in tqdm(range(len(file))):\n",
    "for i in tqdm(range(0, 73)):\n",
    "    # base_image, label_image = process(path, i, model, mini_model, adaptive_crop=True)\n",
    "    base_image, label_image, patch_layer, patch_coord, wanted_probabilities, wanted_images = process(path, i, model, mini_model, \n",
    "                                                                                                     crop_size=(240,240),\n",
    "                                      adaptive_crop=True, overlap=False, stride_proportion=0.1, wanted_patches=[],\n",
    "                                      background_vote=True)\n",
    "    base_stack.append(base_image)\n",
    "    label_stack.append(label_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a5cea28",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_stack_array = np.array(base_stack)\n",
    "label_stack_array = np.array(label_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84089f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import napari\n",
    "viewer = napari.Viewer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4b888ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\miniconda3\\envs\\cell\\Lib\\site-packages\\napari\\utils\\colormaps\\colormap.py:435: UserWarning: color_dict did not provide a default color. Missing keys will be transparent. To provide a default color, use the key `None`, or provide a defaultdict instance.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "viewer.add_image(base_stack_array)\n",
    "\n",
    "colors = {1: '#FF9B9B', 2: '#FFD89C', 3: '#B99470', 4: '#3B6790', 5: \"#98D8EF\"}\n",
    "labels_layer = viewer.add_labels(label_stack_array.astype(int), colormap=colors)\n",
    "labels_layer.color_mode = 'direct'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a383ac7",
   "metadata": {},
   "source": [
    "<h1> Fluorescence one picture </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "804fc633",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Image layer 'ch0_stack [1]' at 0x2920443b800>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from TCFile import TCFile\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "time = 0\n",
    "base = base_stack_array[0].copy()\n",
    "# viewer.add_image(base_stack)\n",
    "\n",
    "colors = ['Blue', 'Green', 'Red']\n",
    "\n",
    "ch0_list = []\n",
    "ch1_list = []\n",
    "ch2_list = []\n",
    "for i in range(1):\n",
    "    file = TCFile(path, '3DFL', channel=i, only_one=True)\n",
    "    temp = np.max(file[time], axis=0)\n",
    "    temp = cv2.resize(temp, dsize=base.shape)\n",
    "    if i==0:\n",
    "        ch0_list.append(temp)\n",
    "    elif i==1:\n",
    "        ch1_list.append(temp)\n",
    "    elif i==2:\n",
    "        ch2_list.append(temp)\n",
    "            \n",
    "ch0_stack = np.array(ch0_list)    \n",
    "ch1_stack = np.array(ch1_list)\n",
    "ch2_stack = np.array(ch2_list)\n",
    "\n",
    "viewer.add_image(base)\n",
    "viewer.add_labels(patch_layer)\n",
    "viewer.add_image(ch0_stack, opacity=0.3, colormap=colors[0])\n",
    "# viewer.add_image(ch1_stack, opacity=0.3, colormap=colors[1])\n",
    "# viewer.add_image(ch2_stack, opacity=0.3, colormap=colors[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b6a1e0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# red = ch2_stack[0]\n",
    "# green = ch1_stack[0]\n",
    "blue = ch0_stack[0]\n",
    "\n",
    "# red = utils.image_normalization(red, min=np.min(red), max=np.max(red))\n",
    "# green = utils.image_normalization(green, min=np.min(green), max=np.max(green))\n",
    "blue = utils.image_normalization(blue, min=np.min(blue), max=np.max(blue))\n",
    "\n",
    "overlay = np.zeros((blue.shape[0], blue.shape[1], 3))\n",
    "# overlay[:,:,0] = red\n",
    "# overlay[:,:,1] = green\n",
    "overlay[:,:,2] = blue\n",
    "overlay = overlay.astype(np.uint8)\n",
    "image = Image.fromarray(overlay)\n",
    "image.save('test.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46a93b71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(248, 248, 248)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patch_coord[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb5e5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch = 192\n",
    "\n",
    "top, left, crop_size = patch_coord[patch]\n",
    "temp = overlay[top:top+crop_size, left:left+crop_size, :].copy()\n",
    "temp = temp.astype(np.uint8)\n",
    "image = Image.fromarray(temp)\n",
    "image.save(f'figures/figure4/FL_patch_{patch}.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cell",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
