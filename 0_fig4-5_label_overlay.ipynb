{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e347cd4",
   "metadata": {},
   "source": [
    "<h1> Inference First </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c143e400",
   "metadata": {},
   "outputs": [],
   "source": [
    "from TCFile import TCFile\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "from torchvision import models\n",
    "import utils\n",
    "\n",
    "def process(path, time, model, mini_model, crop_size=(160,160), adaptive_crop=False, overlap=True, stride_proportion=0.5,\n",
    "            wanted_patches=[], background_vote=False):\n",
    "    # Load                                                                                    & prep data\n",
    "    file = TCFile(path, '2DMIP')\n",
    "    crop_size = crop_size\n",
    "\n",
    "    slice_2d = utils.resize_tomogram_mip(\n",
    "        file[time],\n",
    "        data_resolution=file.data_resolution,\n",
    "        target_resolution=0.1632,\n",
    "        mode='mip'\n",
    "    )\n",
    "\n",
    "    if adaptive_crop:\n",
    "        # Adjust the patch size if you want the patches to fit the image nicely\n",
    "        residual = slice_2d.shape[0] % crop_size[0]\n",
    "        fit = slice_2d.shape[0] // crop_size[0]\n",
    "        adapt = residual // fit if fit > 0 else 0\n",
    "        crop_size = (crop_size[0] + adapt, crop_size[1] + adapt)\n",
    "        stride = int(crop_size[0]*stride_proportion)\n",
    "\n",
    "    base_image = slice_2d.copy()\n",
    "    # base_image = utils.image_normalization(base_image, min=1.33, max=1.4)\n",
    "\n",
    "    # Normal PyTorch model prep\n",
    "    transform = models.ResNet101_Weights.IMAGENET1K_V2.transforms()\n",
    "    mini_transform = models.ResNet50_Weights.IMAGENET1K_V2.transforms()\n",
    "\n",
    "    model = model.cuda().eval()\n",
    "    mini_model = mini_model.cuda().eval()\n",
    "\n",
    "    # We'll store class votes for each pixel here: (H, W, num_classes)\n",
    "    num_classes = 1+5  # Adjust if you actually have a different number\n",
    "    label_counts = np.zeros((base_image.shape[0], base_image.shape[1], num_classes), dtype=np.int32)\n",
    "    patch_layer = np.zeros((base_image.shape[0], base_image.shape[1]), dtype=np.int32)\n",
    "    \n",
    "    patch_coord = []\n",
    "    wanted_patches = wanted_patches\n",
    "    wanted_probabilities = []\n",
    "    wanted_images = []\n",
    "    \n",
    "    if overlap:\n",
    "        # Overlapping patches: define a smaller stride\n",
    "        # e.g. half the patch size in each dimension\n",
    "        stride_h = stride_w = stride\n",
    "        patch_num = 0\n",
    "\n",
    "        for top in range(0, base_image.shape[0] - crop_size[0] + 1, stride_h):\n",
    "            for left in range(0, base_image.shape[1] - crop_size[1] + 1, stride_w):\n",
    "                patch_coord.append((top, left, crop_size[0]))\n",
    "                # Extract patch\n",
    "                cropped = base_image[top:top+crop_size[0], left:left+crop_size[1]]\n",
    "\n",
    "                # Clamp intensities (like your code)\n",
    "                cropped = np.clip(cropped, 1.33, 1.40)\n",
    "                cropped = utils.image_normalization(cropped, min=1.33, max=1.40)\n",
    "\n",
    "                # Decide if we run the main model or the mini_model logic\n",
    "                temp = cropped.copy()\n",
    "                temp[temp < 45] = 0\n",
    "                temp[temp > 45] = 1\n",
    "                proportion = np.count_nonzero(temp == 1) / (np.count_nonzero(temp == 0) + np.count_nonzero(temp == 1))\n",
    "\n",
    "                if proportion < 0.05:\n",
    "                    # Use mini_model to check necrosis\n",
    "                    mini_temp = cv2.equalizeHist(cropped.astype(np.uint8))\n",
    "                    mini_temp = torch.from_numpy(mini_temp).repeat(3, 1, 1).float()\n",
    "                    mini_temp = mini_transform(mini_temp).cuda()\n",
    "\n",
    "                    is_necrosis = torch.max(mini_model(mini_temp.unsqueeze(0)), 1)[1].item()\n",
    "\n",
    "                    if is_necrosis == 1:\n",
    "                        # If necrosis, run the main model\n",
    "                        image_tensor = torch.from_numpy(cropped).repeat(3, 1, 1).float()\n",
    "                        image_tensor = transform(image_tensor).cuda()\n",
    "                        output = model(image_tensor.unsqueeze(0)).cpu()\n",
    "                        _, pred = torch.max(output, 1)\n",
    "                        class_label = pred.item() + 1  # 0..(num_classes-1)\n",
    "                    else:\n",
    "                        class_label = 0  # or some \"background\" label\n",
    "                else:\n",
    "                    # Directly run main model\n",
    "                    image_tensor = torch.from_numpy(cropped).repeat(3, 1, 1).float()\n",
    "                    image_tensor = transform(image_tensor).cuda()\n",
    "                    output = model(image_tensor.unsqueeze(0)).cpu()\n",
    "                    _, pred = torch.max(output, 1)\n",
    "                    class_label = pred.item() + 1\n",
    "                    \n",
    "                \n",
    "                if patch_num in wanted_patches:\n",
    "                    wanted_probabilities.append(output.detach().numpy())\n",
    "                    wanted_images.append(base_image[top:top+crop_size[0], left:left+crop_size[1]])\n",
    "\n",
    "                # Add a vote for this class in label_counts\n",
    "                label_counts[top:top+crop_size[0], left:left+crop_size[1], class_label] += 1\n",
    "                patch_layer[top:top+crop_size[0], left:left+crop_size[1]] = patch_num\n",
    "                patch_num += 1\n",
    "                \n",
    "    else:\n",
    "        # Original no-overlap approach\n",
    "        patches = utils.crop_patch(slice_2d, crop_size=crop_size, overlap=False)\n",
    "        patch_num = 0\n",
    "        for patch_index, patch in enumerate(patches):\n",
    "            top, left = patch[0], patch[1]\n",
    "            patch_coord.append((top, left, crop_size[0]))\n",
    "            cropped = base_image[top:top+crop_size[0], left:left+crop_size[1]]\n",
    "\n",
    "            cropped = np.clip(cropped, 1.33, 1.40)\n",
    "            cropped = utils.image_normalization(cropped, min=1.33, max=1.40)\n",
    "\n",
    "            temp = cropped.copy()\n",
    "            temp[temp < 45] = 0\n",
    "            temp[temp > 45] = 1\n",
    "            proportion = np.count_nonzero(temp == 1) / (np.count_nonzero(temp == 0) + np.count_nonzero(temp == 1))\n",
    "\n",
    "            if proportion < 0.05:\n",
    "                mini_temp = cv2.equalizeHist(cropped.astype(np.uint8))\n",
    "                mini_temp = torch.from_numpy(mini_temp).repeat(3, 1, 1).float()\n",
    "                mini_temp = mini_transform(mini_temp).cuda()\n",
    "                is_necrosis = torch.max(mini_model(mini_temp.unsqueeze(0)), 1)[1].item()\n",
    "\n",
    "                if is_necrosis == 1:\n",
    "                    image_tensor = torch.from_numpy(cropped).repeat(3, 1, 1).float()\n",
    "                    image_tensor = transform(image_tensor).cuda()\n",
    "                    output = model(image_tensor.unsqueeze(0)).cpu()\n",
    "                    _, pred = torch.max(output, 1)\n",
    "                    class_label = pred.item() + 1\n",
    "                else:\n",
    "                    class_label = 0\n",
    "            else:\n",
    "                image_tensor = torch.from_numpy(cropped).repeat(3, 1, 1).float()\n",
    "                image_tensor = transform(image_tensor).cuda()\n",
    "                output = model(image_tensor.unsqueeze(0)).cpu()\n",
    "                _, pred = torch.max(output, 1)\n",
    "                class_label = pred.item() + 1\n",
    "                \n",
    "                \n",
    "            if patch_num in wanted_patches:\n",
    "                wanted_probabilities.append(output.detach().numpy())\n",
    "                wanted_images.append(base_image[top:top+crop_size[0], left:left+crop_size[1]])\n",
    "                \n",
    "            label_counts[top:top+crop_size[0], left:left+crop_size[1], class_label] += 1\n",
    "            patch_layer[top:top+crop_size[0], left:left+crop_size[1]] = patch_num\n",
    "            patch_num += 1\n",
    "    # Final pixelwise label = most frequent vote + 1 (if you want 1-based classes)\n",
    "    # label_image = np.argmax(label_counts, axis=-1)\n",
    "    \n",
    "    # --- MODIFIED FINAL LABEL ASSIGNMENT ---\n",
    "    # Final pixelwise label assignment with background priority\n",
    "\n",
    "    # Calculate the standard argmax first (most frequent vote overall)\n",
    "    provisional_label_image = np.argmax(label_counts, axis=-1)\n",
    "\n",
    "    if background_vote:\n",
    "        # Identify pixels where at least one patch voted for background (class 0)\n",
    "        # label_counts[:, :, 0] accesses the counts for class 0 for all pixels\n",
    "        total = np.sum(label_counts, axis=2)\n",
    "        has_background_vote = 0.15 < label_counts[:, :, 0]/total\n",
    "\n",
    "        # Initialize the final label image with the provisional result\n",
    "        label_image = provisional_label_image.copy()\n",
    "\n",
    "        # Override: wherever a background vote exists, set the final label to 0\n",
    "        label_image[has_background_vote] = 0\n",
    "        # --- END OF MODIFIED SECTION ---\n",
    "    else:\n",
    "        label_image = provisional_label_image.copy()\n",
    "\n",
    "    return base_image, label_image, patch_layer, patch_coord, wanted_probabilities, wanted_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59e98e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\miniconda3\\envs\\cell\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\miniconda3\\envs\\cell\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "c:\\miniconda3\\envs\\cell\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "  0%|          | 0/73 [00:00<?, ?it/s]C:\\Users\\mw\\AppData\\Local\\Temp\\ipykernel_21764\\919404517.py:164: RuntimeWarning: invalid value encountered in divide\n",
      "  has_background_vote = 0.15 < label_counts[:, :, 0]/total\n",
      "100%|██████████| 73/73 [11:08<00:00,  9.16s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torchvision import models\n",
    "import utils\n",
    "import numpy as np\n",
    "\n",
    "path = r\"C:\\rkka_Projects\\cell_death_v2\\Data\\9. A549_FasL(20250410)\\250409.170229.A549_FasL_01.025.Group3.B5.T025P03.TCF\"\n",
    "class_num = 5\n",
    "model_path = r\"C:\\rkka_Projects\\cell_death_v2\\trained_models\\test_5_classes_22.032991_0.9728_sota.pth\"\n",
    "mini_model_path = r\"C:\\rkka_Projects\\cell_death_v2\\trained_models\\mini_ai_epoch_9_0.000861_1.0000.pth\"\n",
    "file = TCFile(path, '2DMIP')\n",
    "# Load Model\n",
    "model = models.resnet101(pretrained=True)\n",
    "num_features = model.fc.in_features\n",
    "model.fc = torch.nn.Sequential(\n",
    "    torch.nn.Dropout(0.2),\n",
    "    torch.nn.Linear(num_features, 5)\n",
    ")\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "# Load mini Model\n",
    "mini_model = models.resnet50(pretrained=True)\n",
    "num_features = mini_model.fc.in_features\n",
    "mini_model.fc = torch.nn.Sequential(\n",
    "    torch.nn.Dropout(0.2),\n",
    "    torch.nn.Linear(num_features, 2)\n",
    ")\n",
    "mini_model.load_state_dict(torch.load(mini_model_path))\n",
    "\n",
    "# Process\n",
    "base_stack = []\n",
    "label_stack = []\n",
    "probabilities_stack = []\n",
    "\n",
    "# wanted_patches = [110, 334, 329, 258, 192]\n",
    "# wanted_patches = [0]\n",
    "\n",
    "# for i in tqdm(range(len(file))):\n",
    "for i in tqdm(range(0, 73)):\n",
    "    # base_image, label_image = process(path, i, model, mini_model, adaptive_crop=True)\n",
    "    base_image, label_image, patch_layer, patch_coord, wanted_probabilities, wanted_images = process(path, i, model, mini_model, \n",
    "                                                                                                     crop_size=(240,240),\n",
    "                                      adaptive_crop=True, overlap=True, stride_proportion=0.1, wanted_patches=[],\n",
    "                                      background_vote=True)\n",
    "    base_stack.append(base_image)\n",
    "    label_stack.append(label_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2792f247",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_stack_array = np.array(base_stack)\n",
    "label_stack_array = np.array(label_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef05496f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import napari\n",
    "viewer = napari.Viewer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1594e944",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Image layer 'base_stack_array' at 0x162c6711190>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "viewer.add_image(base_stack_array)\n",
    "\n",
    "colors = {1: '#FF9B9B', 2: '#FFD89C', 3: '#B99470', 4: '#3B6790', 5: \"#98D8EF\"}\n",
    "labels_layer = viewer.add_labels(label_stack_array.astype(int), colormap=colors)\n",
    "labels_layer.color_mode = 'direct'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a35ad0f",
   "metadata": {},
   "source": [
    "<h1> Label overlay </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d1fe25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib.colors import ListedColormap\n",
    "import utils\n",
    "from PIL import Image\n",
    "\n",
    "time = 0\n",
    "patch = 110\n",
    "# Load grayscale base image\n",
    "# path = r\"C:\\rkka_Projects\\cell_death_v2\\Data\\Large_FOV\\240808.200653.death_B4C4B5.005.Group2.C4.T001P01.TCF\"\n",
    "# base = TCFile(path, '2DMIP')\n",
    "# base_stack_array = np.array([b for b in base])[:33]\n",
    "# Load grayscale base image\n",
    "\n",
    "ri_max = np.max(base_stack_array[time])\n",
    "base_stack_array = np.array(base_stack)\n",
    "base = utils.image_normalization(base_stack_array[time], min=1.33, max=1.42)  # Ensure it's in the correct format\n",
    "\n",
    "# base = base[0:960, 0:960]\n",
    "# Load label data\n",
    "label = label_stack_array[time]\n",
    "\n",
    "# label = label[0:960, 0:960]\n",
    "# Define colors for labels\n",
    "# colors = {0: (0, 0, 0), 1: (255, 155, 155), 2: (255, 216, 156), \n",
    "#           3: (185, 148, 112), 4: (59, 103, 144), 5: (152, 216, 239)}\n",
    "\n",
    "colors = {0: (0, 0, 0), 1: (255, 155, 155), 2: (255, 216, 156), \n",
    "          3: (185, 148, 112), 4: (152, 216, 239), 5: (152, 216, 239)}\n",
    "\n",
    "# Create a blank color image to store overlay\n",
    "overlay = np.zeros((*label.shape, 3), dtype=np.uint8)\n",
    "\n",
    "# Assign colors to labels\n",
    "for lbl, color in colors.items():\n",
    "    overlay[label == lbl] = color\n",
    "\n",
    "# Blend the label overlay with the grayscale image\n",
    "alpha = 0.4  # Transparency factor\n",
    "blended = cv2.addWeighted(cv2.cvtColor(base, cv2.COLOR_GRAY2BGR), 1, overlay, alpha, 0)\n",
    "\n",
    "# Save the final overlaid image\n",
    "image = Image.fromarray(blended)\n",
    "image.save(f'figures/figure5/whole_t_{time//2}h.png')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
